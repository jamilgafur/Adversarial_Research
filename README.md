# Adversraial Observation
---

The usage of neural networks in safety-critical and other domains has raised concerns about their reliability and interpretability. The lack of methods to derive reliable and explainable outputs from these networks presents a significant challenge in understanding their decision-making processes. To address this challenge, the authors propose a new technique called "Adversarial Observation" that leverages adversarial attacks to infer relationships between input and output predictions in a human-understandable format.

In this study, the authors demonstrate the effectiveness of this technique by applying the Fast Gradient Sign Method and the Adversarial Particle Swarm Optimizer to two separate problems. The results show that adversarial attacks can reveal insights into the learned input features and their correlations to various predictions made by the network.

Overall, this study sheds light on the potential of adversarial attacks as a means to enhance the reliability and explainability of neural networks, which could have important implications for a range of industries and applications.


